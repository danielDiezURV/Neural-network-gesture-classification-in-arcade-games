{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd8feaba",
   "metadata": {},
   "source": [
    "# Data Generation for Hand Gesture Recognition\n",
    "\n",
    "This notebook is the first step in our gesture recognition pipeline. Its purpose is to collect the raw data needed to train our neural network. It provides a streamlined process for capturing a large number of images for various hand gestures directly from a webcam.\n",
    "\n",
    "The key outcomes of this notebook are:\n",
    "1.  A structured directory of images, with each sub-directory corresponding to a specific gesture.\n",
    "2.  Two CSV files (`gestures_dataset_train.csv` and `gestures_dataset_val.csv`) containing processed hand landmark data, ready for model training.\n",
    "3.  A default key bindings configuration file (`key_bindings_default.json`) that maps gestures to keyboard actions.\n",
    "\n",
    "### Table of Contents\n",
    "1. [Setup and Dependencies](#setup)\n",
    "2. [Configuration for Image Capture](#config)\n",
    "3. [Live Image Capture](#capture)\n",
    "4. [Processing Images into a Dataset](#processing)\n",
    "5. [Results and Next Steps](#results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cc0346",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 1. Setup and Dependencies\n",
    "\n",
    "This section ensures that the environment is correctly configured to run the notebook. The following code cell installs all the necessary Python libraries listed in the `requirements.txt` file.\n",
    "\n",
    "**How to Use:**\n",
    "1.  Make sure you have Python and `pip` installed.\n",
    "2.  Run the next cell to install all dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67e8ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a5384d",
   "metadata": {},
   "source": [
    "<a id=\"config\"></a>\n",
    "## 2. Configuration for Image Capture\n",
    "\n",
    "Before we begin capturing images, we need to define a few parameters. The next cell allows you to customize the data collection process.\n",
    "\n",
    "### Parameters:\n",
    "-   `num_screenshots`: The total number of images to capture for the specified gesture. A larger number of diverse images will lead to a more robust model.\n",
    "-   `capture_rate`: The time delay (in seconds) between each screenshot. A small delay helps in capturing slight variations of the gesture.\n",
    "-   `gesture_name`: The name of the gesture you are capturing (e.g., \"fist\", \"palm_up\", \"nice\"). This name is critical as it will be used as the class label for the gesture. It also determines the folder where the images will be saved.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Decide on a gesture you want to add (e.g., \"thumb_up\").\n",
    "2.  Set the `gesture_name` variable to your chosen name.\n",
    "3.  Run the cell to apply the configuration.\n",
    "4.  Repeat this process for every new gesture you want to add to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfed6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "num_screenshots = 250  # Number of images to capture\n",
    "capture_rate = 0.1  # Seconds between captures\n",
    "gesture_name = \"thump_up\"  # Name of the gesture (e.g., \"fist\", \"point_right\")\n",
    "\n",
    "# --- File and Folder Setup ---\n",
    "base_folder = \"data/gestures\"\n",
    "folder_location = os.path.join(base_folder, gesture_name)\n",
    "picture_name = gesture_name\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(folder_location, exist_ok=True)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\" - Number of screenshots: {num_screenshots}\")\n",
    "print(f\" - Capture rate: {capture_rate} seconds\")\n",
    "print(f\" - Gesture name: '{gesture_name}'\")\n",
    "print(f\" - Saving to: '{folder_location}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7152acaa",
   "metadata": {},
   "source": [
    "<a id=\"capture\"></a>\n",
    "## 3. Live Image Capture\n",
    "\n",
    "This section contains the code to capture images from your webcam.\n",
    "\n",
    "### How It Works:\n",
    "-   The script opens a window displaying your webcam feed.\n",
    "-   It's designed for a \"hands-free\" operation after the initial trigger.\n",
    "-   Position your hand to form the gesture you configured in the previous step.\n",
    "-   Press the **'s'** key to start the automated capture sequence. The window will show a countdown of the images being taken.\n",
    "-   Slightly move your hand and change its orientation between captures to create a varied dataset.\n",
    "-   Press the **'q'** key at any time to stop the capture and close the window.\n",
    "\n",
    "The captured images will be saved in the `data/gestures/<gesture_name>/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1a8b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import os\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "count = len(os.listdir(folder_location))\n",
    "\n",
    "screenshots_taken = 0\n",
    "capturing = False\n",
    "last_capture_time = 0\n",
    "window_name = 'Data Generation'\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    display_frame = frame.copy()\n",
    "\n",
    "    if not capturing:\n",
    "        cv2.putText(display_frame, \"Press 's' to start capturing\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "    else:\n",
    "        cv2.putText(display_frame, f\"Capturing... {screenshots_taken}/{num_screenshots}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "    \n",
    "    cv2.putText(display_frame, \"Press 'q' to quit\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "\n",
    "    cv2.imshow(window_name, display_frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    elif key == ord('s'):\n",
    "        if not capturing:\n",
    "            print(\"Starting capture...\")\n",
    "            capturing = True\n",
    "            last_capture_time = time.time()\n",
    "\n",
    "    if capturing:\n",
    "        current_time = time.time()\n",
    "        if current_time - last_capture_time >= capture_rate:\n",
    "            if screenshots_taken < num_screenshots:\n",
    "                img_name = os.path.join(folder_location, f\"{picture_name}_{count:03d}.jpg\")\n",
    "                cv2.imwrite(img_name, frame)\n",
    "                print(f\"Saved {img_name}\")\n",
    "                count += 1\n",
    "                screenshots_taken += 1\n",
    "                last_capture_time = current_time\n",
    "            else:\n",
    "                print(\"Capture complete.\")\n",
    "                break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9ef4f0",
   "metadata": {},
   "source": [
    "<a id=\"processing\"></a>\n",
    "## 4. Processing Images into a Dataset\n",
    "\n",
    "After capturing the raw images, this section processes them to create a structured, machine-learning-ready dataset.\n",
    "\n",
    "### The Processing Pipeline:\n",
    "\n",
    "1.  **Load Configuration**: Loads paths and settings from the `app_config.json` file.\n",
    "2.  **Initialize Tools**: Sets up the `GestureDetector` (powered by MediaPipe) to find hand landmarks and the `DataPreprocessor` for normalizing them.\n",
    "3.  **Map Gestures to IDs**: It scans the `data/gestures/` directory, assigning a unique integer ID to each gesture folder (e.g., 'fist': 0, 'palm_up': 1).\n",
    "4.  **Extract Landmarks**: The code iterates through every image in each gesture folder. For each image:\n",
    "    *   It uses `GestureDetector` to locate the 21 hand landmarks.\n",
    "    *   If a hand is found, the `DataPreprocessor` normalizes the landmark coordinates. Normalization is crucial as it makes the model robust to variations in hand size, position, and rotation. It involves translating the landmarks relative to the wrist and scaling them.\n",
    "5.  **Create DataFrame**: The processed data (gesture ID and the list of normalized landmarks) is compiled into a pandas DataFrame.\n",
    "6.  **Train-Validation Split**: The dataset is split into a training set (90%) and a validation set (10%). We use stratified sampling (`stratify=df['GESTURE_ID']`) to ensure that the proportion of each gesture is the same in both the training and validation sets. This prevents bias and leads to more reliable evaluation.\n",
    "7.  **Save Datasets**: The final training and validation DataFrames are saved as CSV files, which will be the direct input for our model training notebook.\n",
    "8.  **Generate Key Bindings**: A default `key_bindings_default.json` file is created. This file acts as a template for mapping the newly created gesture IDs to keyboard actions in the main application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0abc6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from src.gesture_controller.data_preprocessor import DataPreprocessor\n",
    "from src.gesture_controller.gesture_detector import GestureDetector\n",
    "from src.gesture_controller.app_config import AppConfig\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "config = AppConfig()\n",
    "data_generation_config = config.get_app_config(\"DATA_GENERATION_CONFIG\")\n",
    "gestures_folder = data_generation_config.get(\"GESTURE_FOLDER\")\n",
    "datase_train_path = data_generation_config.get(\"DATASET_TRAINING_PATH\")\n",
    "datase_val_path = data_generation_config.get(\"DATASET_VAL_PATH\")\n",
    "key_bindings_file = data_generation_config.get(\"KEY_BINDING_OUTPUT_FILE\")\n",
    "\n",
    "# --- Initialization ---\n",
    "# We set max_hands to 1 because we are analyzing images of a single gesture\n",
    "detector = GestureDetector(max_hands=1, min_detection_confidence=0.5)\n",
    "data_preprocessor = DataPreprocessor()\n",
    "dataset = []\n",
    "\n",
    "# --- Data Processing ---\n",
    "\n",
    "# Get the list of gesture subdirectories and create a mapping to integer IDs\n",
    "gesture_folders = sorted([f for f in os.listdir(gestures_folder) if os.path.isdir(os.path.join(gestures_folder, f))])\n",
    "gesture_map = {name: i for i, name in enumerate(gesture_folders)}\n",
    "\n",
    "print(\"Processing gestures...\")\n",
    "print(f\"Found gestures: {gesture_map}\")\n",
    "\n",
    "# Iterate over each gesture folder\n",
    "for gesture_name, gesture_id in gesture_map.items():\n",
    "    folder_path = os.path.join(gestures_folder, gesture_name)\n",
    "    print(f\"\\nProcessing folder: {folder_path}\")\n",
    "\n",
    "    # Get a sorted list of images to process them in a consistent order\n",
    "    image_files = sorted([f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "    # Iterate over each image in the folder\n",
    "    for image_name in image_files:\n",
    "        image_path = os.path.join(folder_path, image_name)\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            print(f\"Warning: Could not read image {image_path}\")\n",
    "            continue\n",
    "\n",
    "        # Process the frame to find hands\n",
    "        detector.process_frame(image)\n",
    "        \n",
    "        # Get hand vectors (we expect only one hand)\n",
    "        left_hand, right_hand = detector.get_hand_vectors()\n",
    "        hand_vector = left_hand or right_hand\n",
    "\n",
    "        if hand_vector:\n",
    "            # Normalize the landmarks\n",
    "            normalized_landmarks = data_preprocessor.process(hand_vector)\n",
    "            \n",
    "            if normalized_landmarks is not None:\n",
    "                # Append the gesture ID and the flattened list of landmarks\n",
    "                dataset.append([gesture_id, normalized_landmarks.tolist()])\n",
    "        else:\n",
    "            print(f\"Warning: No hand detected in {image_path}\")\n",
    "\n",
    "# --- Generate Default Key Bindings Configuration ---\n",
    "key_bindings_config = {}\n",
    "for gesture_name, gesture_id in gesture_map.items():\n",
    "    key_bindings_config[str(gesture_id)] = {\n",
    "        \"gesture\": gesture_name,\n",
    "        \"keys\": [],\n",
    "        \"behavior\": \"\"\n",
    "    }\n",
    "\n",
    "# Save the default key bindings configuration\n",
    "with open(key_bindings_file, 'w') as f:\n",
    "    json.dump(key_bindings_config, f, indent=2)\n",
    "\n",
    "# --- Create and Save DataFrames ---\n",
    "# Create a DataFrame from the collected data\n",
    "df = pd.DataFrame(dataset, columns=[\"GESTURE_ID\", \"LANDMARKS\"])\n",
    "\n",
    "# Split the dataset into training and validation sets (90% train, 10% val)\n",
    "# We stratify by GESTURE_ID to ensure that the distribution of gestures is\n",
    "# similar in both the training and validation sets.\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=df['GESTURE_ID']\n",
    ")\n",
    "\n",
    "# Save the datasets to CSV files\n",
    "train_df.to_csv(datase_train_path, index=False)\n",
    "val_df.to_csv(datase_val_path, index=False)\n",
    "\n",
    "print(f\"\\nDatasets created successfully.\")\n",
    "print(f\"Training set: {len(train_df)} samples, saved to '{datase_train_path}'\")\n",
    "print(f\"Validation set: {len(val_df)} samples, saved to '{datase_val_path}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d98f741",
   "metadata": {},
   "source": [
    "<a id=\"results\"></a>\n",
    "## 5. Results and Next Steps\n",
    "\n",
    "The successful execution of this notebook produces the following artifacts:\n",
    "\n",
    "-   **Image Folders**: A collection of gesture images located in `data/gestures/`.\n",
    "-   **Training Data**: `data/gestures_dataset_train.csv`\n",
    "-   **Validation Data**: `data/gestures_dataset_val.csv`\n",
    "-   **Key Binding Configuration**: `config/key_bindings_default.json`\n",
    "\n",
    "### Key Bindings Configuration\n",
    "\n",
    "The generated `key_bindings_default.json` file is a crucial part of the application. You need to **manually edit this file** to map the detected gestures to specific keyboard actions.\n",
    "\n",
    "For each gesture, you can specify:\n",
    "-   `keys`: A list of keyboard keys to be triggered. For example: `[\"ctrl\", \"c\"]` for copy, or `[\"up\"]` for moving up.\n",
    "-   `behavior`: The action to perform with the keys. This can be one of two values:\n",
    "    -   `\"press\"`: Simulates a quick press and release of the keys (e.g., for firing a weapon).\n",
    "    -   `\"hold\"`: Simulates pressing and holding the keys down (e.g., for continuous movement). The keys will be released when the gesture is no longer detected.\n",
    "\n",
    "### Next Steps\n",
    "With the data generated and processed, you are now ready to proceed to the `neural_network_training.ipynb` notebook to train the gesture classification model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
