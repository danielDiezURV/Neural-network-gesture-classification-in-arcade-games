{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea1de4ef",
   "metadata": {},
   "source": [
    "# Hand Gesture Recognition Notebook\n",
    "\n",
    "This notebook uses MediaPipe to detect hand landmarks from a webcam feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac43d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619e6142",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "This section loads the dataset created in the `data_generation` notebook, splits it into training and testing sets, and then trains a neural network to classify the gestures. The trained model is saved for later use in real-time prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff42e64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gesture_classifier import GestureClassifier\n",
    "from config.app_config import AppConfig\n",
    "import itertools\n",
    "\n",
    "# Create a default AppConfig instance for training (uses default key_bindings.json)\n",
    "app_config = AppConfig()\n",
    "\n",
    "# --- Load Configuration ---\n",
    "model_path = app_config.get_neural_network_config().get('MODEL_PATH')\n",
    "dataset_path = app_config.get_neural_network_config().get('DATASET_PATH')\n",
    "hyperparam_config = app_config.get_hyperparameter_config()\n",
    "\n",
    "# --- Load and Prepare Dataset ---\n",
    "try:\n",
    "    df = pd.read_csv(dataset_path)\n",
    "\n",
    "    # Convert the 'LANDMARKS' column from string representation of list to actual list\n",
    "    df['LANDMARKS'] = df['LANDMARKS'].apply(eval)\n",
    "\n",
    "    X = np.array(df['LANDMARKS'].tolist())\n",
    "    Y = df['GESTURE_ID']\n",
    "\n",
    "    # Dynamically determine number of classes based on highest gesture ID\n",
    "    max_gesture_id = Y.max()\n",
    "    num_classes = max_gesture_id + 1\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "        X, Y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # --- Hyperparameter Tuning ---\n",
    "    hyperparam_keys = list(hyperparam_config.keys())\n",
    "    hyperparam_values = list(hyperparam_config.values())\n",
    "\n",
    "    # Generate all combinations\n",
    "    hyperparam_combinations = list(itertools.product(*hyperparam_values))\n",
    "\n",
    "    print(f\"Total hyperparameter combinations: {len(hyperparam_combinations)}\")\n",
    "\n",
    "    training_history = []\n",
    "    best_model = None\n",
    "    best_val_accuracy = 0\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # Iterate over all hyperparameter combinations\n",
    "    for i, params in enumerate(hyperparam_combinations):\n",
    "        \n",
    "        hyperparams = dict(zip(hyperparam_keys, params))\n",
    "        \n",
    "        print(f\"--- Training Variant {i+1}/{len(hyperparam_combinations)} ---\")\n",
    "        print(f\"Hyperparameters: {hyperparams}\")\n",
    "        \n",
    "        # Create a new classifier for each hyperparameter combination\n",
    "        classifier = GestureClassifier(num_classes=num_classes, input_size=X_train.shape[1], hyperparams=hyperparams)\n",
    "\n",
    "        # Train the model\n",
    "        model, history = classifier.train(\n",
    "            X_train, Y_train, X_val, Y_val, \n",
    "            batch_size=hyperparams['BATCH_SIZE'], \n",
    "            epochs=hyperparams['EPOCHS']\n",
    "        )\n",
    "        \n",
    "        # Get final validation metrics\n",
    "        score_val_accuracy = history.history['val_accuracy'][-1]\n",
    "        score_val_loss = history.history['val_loss'][-1]\n",
    "        \n",
    "        # Store history\n",
    "        run_info = {\n",
    "            'variant': i + 1,\n",
    "            'hyperparameters': hyperparams,\n",
    "            'val_accuracy': score_val_accuracy,\n",
    "            'val_loss': score_val_loss,\n",
    "            'history': history.history,\n",
    "            'model': model\n",
    "        }\n",
    "        training_history.append(run_info)\n",
    "        \n",
    "\n",
    "    # Display results\n",
    "    history_df = pd.DataFrame(training_history)\n",
    "    history_df.to_csv('models/models_performance.csv', index=False)\n",
    "\n",
    "    best_run = min(training_history, key=lambda x: (1 - x['val_accuracy'], x['val_loss']))\n",
    "    print(\"Best model found with validation accuracy:\", best_run['val_accuracy'], \"and validation loss:\", best_run['val_loss'])\n",
    "    best_run['model'].save(model_path)\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Dataset file not found at '{dataset_path}'.\")\n",
    "    print(\"Please run the data generation notebook first.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291ca48f",
   "metadata": {},
   "source": [
    "## Variant Performance Analysis\n",
    "\n",
    "This section provides a concise, visual summary of how different hyperparameter configurations impact validation performance. Using the aggregated `history_df` and per-run `training_history`, plots will compare and rank variants to support model selection.\n",
    "\n",
    "Planned visuals:\n",
    "- Overview\n",
    "    - Line/scatter: variant vs. val_accuracy and val_loss (with thresholds)\n",
    "    - Leaderboard: top-N variants by val_accuracy, tie-broken by val_loss\n",
    "- Distributions and stability\n",
    "    - Box/violin plots of val_accuracy and val_loss grouped by:\n",
    "        - ACTIVATION\n",
    "        - DROPOUT_RATE\n",
    "        - LEARNING_RATE\n",
    "        - BATCH_SIZE\n",
    "        - DENSE_LAYERS depth/width\n",
    "- Interactions\n",
    "    - Faceted scatter: LEARNING_RATE Ã— DROPOUT_RATE, colored by ACTIVATION, sized by BATCH_SIZE\n",
    "    - Parallel coordinates across hyperparameters with color mapped to val_accuracy\n",
    "- Learning dynamics\n",
    "    - Training vs. validation accuracy/loss curves for the best and a few representative variants to inspect generalization gap\n",
    "- Sensitivity\n",
    "    - Bar charts of mean val_accuracy by each hyperparameter level with error bars\n",
    "    - Pairwise comparisons to highlight robust regions of the search space\n",
    "\n",
    "Outcomes:\n",
    "- Identify the best-performing and most stable configurations\n",
    "- Reveal hyperparameter sensitivities and beneficial combinations\n",
    "- Choose a final model to save to MODEL_PATH and document recommended defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85931dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_helper import PlotHelper\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('models/models_performance.csv')\n",
    "plot_helper = PlotHelper()\n",
    "plot_helper.plot_model_val_loss_score(df)\n",
    "plot_helper.plot_val_loss_by_parameter(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafc71b4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b118fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from plot_helper import PlotHelper\n",
    "from sklearn.model_selection import train_test_split\n",
    "from config.app_config import AppConfig\n",
    "\n",
    "df = pd.read_csv('models/models_performance.csv')\n",
    "winner = df.sort_values(by=['val_accuracy', 'val_loss'], ascending=[False, True]).iloc[0]\n",
    "plot_helper = PlotHelper()\n",
    "plot_helper.plot_val_loss_in_epochs(winner)\n",
    "plot_helper.plot_val_accuracy_in_epochs(winner)\n",
    "\n",
    "# Resolve paths from config\n",
    "cfg = AppConfig()\n",
    "paths = cfg.get_neural_network_config()\n",
    "dataset_path = paths.get('DATASET_PATH')\n",
    "model_path = paths.get('MODEL_PATH')\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(dataset_path)\n",
    "df['LANDMARKS'] = df['LANDMARKS'].apply(eval)\n",
    "X = np.array(df['LANDMARKS'].tolist())\n",
    "Y = df['GESTURE_ID'].to_numpy()\n",
    "\n",
    "# Train/val split (same seed as training)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.9, random_state=42)\n",
    "\n",
    "# Load model and predict\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "probs = model.predict(X_val, verbose=0)\n",
    "y_pred = np.argmax(probs, axis=1)\n",
    "\n",
    "# Confusion matrix plot\n",
    "classes = np.unique(Y)\n",
    "plot_helper.plot_heatmap(Y_val, y_pred, classes)\n",
    "\n",
    "# ROC plot\n",
    "plot_helper.plot_roc_curve(Y_val, probs, classes)\n",
    "\n",
    "# PCA Visualization plot\n",
    "plot_helper.plot_pca(X_val, y_pred)\n",
    "\n",
    "# t-SNE Visualization plot\n",
    "plot_helper.plot_tsne(X_val, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
